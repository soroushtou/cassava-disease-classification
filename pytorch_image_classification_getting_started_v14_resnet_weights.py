# -*- coding: utf-8 -*-
"""pytorch_image_classification_getting_started_V14_Resnet_weights.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WFseeQczFNdf0WxzxLw-4CDq4c2Qf5QH
"""

!pip3 install torchsummary
!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
!pip3 install torchtext --upgrade
!pip3 install fastai --upgrade

import os
import pandas as pd
from torch.utils.data import Dataset,Subset
from torchvision import transforms,models,datasets
import torch
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
from torchvision.utils import make_grid
from torchsummary import summary
from sklearn.metrics import confusion_matrix
import itertools
import tensorflow as tf
import tensorflow_hub as hub
import shutil

os.chdir('C:/Local Machine/Cassava plant disease detection')

! pip install kaggle
! mkdir ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

!kaggle competitions download cassava-leaf-disease-classification -f train.csv
!kaggle competitions download cassava-leaf-disease-classification -f label_num_to_disease_map.json
url = 'https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/13836/1718836/compressed/train_images.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1632816375&Signature=ZcHK%2ByVWK%2BksI9Ab6P2skLxtp6wFT6X4xHG9mcER3C4lBYjk9q7JXP6pF7G4OJJ%2FFiHYNhLTq7LibgS8LyurmQYWSPCAHpPiNIkeTYEG44gLBKH4Qx3IrXyMQ1WK4mmh%2BhUtjOMJqAYQp85dcXsYScwP2ipXmiU8pOoWOf%2BZPQFuGpr2yMCImJk%2FAdOEaiZhvr4AmMwKUpBJ82lc4YlTTmblqpPIPpDE9ZGrKiSDLwtEfbLykXrw%2Fxc9fuMvGGivxs3nfJt3vJddoPCU2EwQRBnaRwKr4KtKYo3ljvVZ4ELDwIIEEb9BAFEi9OHMNx%2Fb6vX6SgD59tjXwYMaPmEIWw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain_images.zip'
!pip install wget
import wget
wget.download(url, 'train.zip')
import zipfile as zf
file0 = zf.ZipFile('train.zip')
file0.extractall('train_images')
#!kaggle competitions download cassava-leaf-disease-classification -f train_images -p train_images

from google.colab import drive
drive.mount('/content/drive')

#os.chdir('/kaggle/input/cassava-leaf-disease-classification')
os.chdir('/content')

# LOAD THE DATASET
train_df = pd.read_csv('train.csv')
train_label = train_df.copy()
# GET THE IMAGE ID NUMBER
label_names = ['Cassava Bacterial Blight (CBB)','Cassava Brown Streak Disease (CBSD)','Cassava Green Mottle (CGM)','Cassava Mosaic Disease (CMD)','Healthy']
label_nums = []
number_of_class = [0,0,0,0,0]
for i in train_label['label']:
  t = int(i)
  number_of_class[int(i)] = number_of_class[int(i)] + 1
  label_nums.append(label_names[t])
train_label['disease'] = label_nums
train_label['label'] = train_label['label'].astype(str)
print(train_label.shape)
print(f"number of data's in each class :{number_of_class}")
print(train_label.head())

class PathologyPlantsDataset(Dataset):  #  <--- جنس پارامتر ورودی از نوع دیتاست برگرفته از کتابخانه دیتاست پایتورچ می باشد
  """
  The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.
  """
  def __init__(self, data_frame, root_dir, transform=None):
        self.data_frame = data_frame
        self.root_dir = root_dir
        self.transform = transform
    
  def __len__(self):
        # Return the length of the dataset
        return len(self.data_frame)
    
  def __getitem__(self, idx):
        # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])
        image = Image.open(img_name)
        label = int(self.data_frame.iloc[idx, 1])
        
        if self.transform:
            image = self.transform(image)
    
        return (image, label)

# INSTANTIATE THE OBJECT
transform = transforms.Compose([transforms.RandomResizedCrop(224),transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),
                                transforms.ColorJitter(brightness=.2, hue=.1),#transforms.RandomAdjustSharpness(sharpness_factor=2),
                                transforms.RandomPerspective(distortion_scale=0.4,p=0.2),
                                transforms.RandomRotation(degrees=(0,180)),#transforms.RandomAffine(),
                                transforms.ToTensor(),#transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
                                ])
pathology_train = PathologyPlantsDataset(data_frame=train_label,root_dir='train_images',transform=transform)
print(pathology_train)

temp_img, temp_lab = pathology_train[4]
plt.imshow(temp_img.numpy().transpose((1, 2, 0)))
plt.title(label_names[int(temp_lab)])
#plt.title(temp_lab)
#plt.axis('off')
plt.show()
print(type(pathology_train))
print(type(temp_lab))

batch_size = 32   #  <------------------
validation_size = 2000
train_size = 15000
test_size = len(pathology_train) - train_size - validation_size

print("Length of Dataset : " + str(len(pathology_train)))
train_data,validation_data,test_data = random_split(pathology_train,[train_size,validation_size,test_size])
print(f"Length of Train Data : {len(train_data)}")
print(f"Length of Validation Data : {len(validation_data)}")
print(f"Length of Test Data : {len(test_data)}")

#load the train and validation into batches.
train_dl = DataLoader(train_data, batch_size, shuffle = False, pin_memory = True)  # , pin_memory = True num_workers = 1, shuffle = True
val_dl = DataLoader(validation_data, batch_size, shuffle = False, pin_memory = True)  # , pin_memory = True num_workers = 1, shuffle = True
test_dl = DataLoader(test_data, batch_size, shuffle = False, pin_memory = True)

def show_batch(dl):
    """Plot images grid of single batch"""
    for images, labels in dl:
        fig,ax = plt.subplots(figsize = (16,12))
        ax.set_xticks([])
        ax.set_yticks([])
        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))
        break

  
show_batch(train_dl)

classifier = hub.KerasLayer('https://tfhub.dev/google/cropnet/classifier/cassava_disease_V1/2')

model = classifier

targets_size = 5
model = models.resnet50(pretrained=True)
for params in model.parameters():
    params.requires_grad = True
print(model)
model.fc = nn.Linear(in_features= 2048 , out_features= 5)
"""
model.add_module('7',nn.Linear(in_features=1000, out_features=320, bias=True))
model.add_module('8',nn.ReLU())
model.add_module('9',nn.Dropout(p=0.4,inplace=True))
model.add_module('10',nn.Linear(in_features=320, out_features=80, bias=True))
model.add_module('11',nn.ReLU())
model.add_module('12',nn.Dropout(p=0.4,inplace=True))
model.add_module('13',nn.Linear(in_features=80, out_features=20, bias=True))
model.add_module('14',nn.ReLU())
model.add_module('15',nn.Dropout(p=0.4,inplace=True))
model.add_module('16',nn.Linear(in_features=20, out_features=5, bias=True))
"""

torch.cuda.reset_peak_memory_stats
torch.cuda.reset_max_memory_allocated
torch.cuda.reset_max_memory_cached
torch.cuda.reset_accumulated_memory_stats
torch.cuda.empty_cache
print(torch.cuda.memory_reserved())

def pretty_size(size):
	"""Pretty prints a torch.Size object"""
	assert(isinstance(size, torch.Size))
	return " × ".join(map(str, size))

def dump_tensors(gpu_only=True):
	"""Prints a list of the Tensors being tracked by the garbage collector."""
	import gc
	total_size = 0
	for obj in gc.get_objects():
		try:
			if torch.is_tensor(obj):
				if not gpu_only or obj.is_cuda:
					print("%s:%s%s %s" % (type(obj).__name__, 
										  " GPU" if obj.is_cuda else "",
										  " pinned" if obj.is_pinned else "",
										  pretty_size(obj.size())))
					total_size += obj.numel()
			elif hasattr(obj, "data") and torch.is_tensor(obj.data):
				if not gpu_only or obj.is_cuda:
					print("%s → %s:%s%s%s%s %s" % (type(obj).__name__, 
												   type(obj.data).__name__, 
												   " GPU" if obj.is_cuda else "",
												   " pinned" if obj.data.is_pinned else "",
												   " grad" if obj.requires_grad else "", 
												   " volatile" if obj.volatile else "",
												   pretty_size(obj.data.size())))
					total_size += obj.data.numel()
		except Exception as e:
			pass        
	print("Total size:", total_size)
 
dump_tensors()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
device = "cuda"
model.to(device)

summary(model, (3, 224, 224))

weights = torch.tensor([0.0,0.0,0.0,0.0,0.0], dtype=torch.float32)
for i in range(len(number_of_class)) :
  weights[i] = number_of_class[i]
weights = weights / weights.sum()
print(weights)
weights = 1.0 / weights
weights = weights / weights.sum()
print(weights)
weights = weights.to(device)

criterion = nn.CrossEntropyLoss(weight = weights)  # this include softmax + cross entropy loss
optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)

train_losses = []
validation_losses = []

def batch_gd(model, criterion, train_loader, validation_loader, epochs):
    #np.append(arr = train_losses,values = np.zeros(epochs))
    #np.append(arr = validation_losses,values = np.zeros(epochs))
    
    
    prev_loss = 0.492
    weights.to(device)
    for e in range(epochs):
        os.chdir('C:/Local Machine/Cassava plant disease detection')
        #os.chdir('/content')    #colab
        #os.chdir('/kaggle/input/cassava-leaf-disease-classification')   #kaggle
        t0 = datetime.now()
        train_loss = []
        n_correct = 0
        n_total = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()

            output = model(inputs)

            loss = criterion(output, targets)

            train_loss.append(loss.item())  # torch to numpy world

            loss.backward()
            optimizer.step()

        train_loss = np.mean(train_loss)

        validation_loss = []

        for inputs, targets in validation_loader:

            inputs, targets = inputs.to(device), targets.to(device)

            output = model(inputs)

            loss = criterion(output, targets)

            validation_loss.append(loss.item())  # torch to numpy world

            _, predictions = torch.max(output, 1)
            n_correct += (predictions == targets).sum().item()
            n_total += targets.shape[0]

        validation_loss = np.mean(validation_loss)

        #train_losses[e] = train_loss
        #validation_losses[e] = validation_loss
        train_losses.append(train_loss)
        validation_losses.append(validation_loss)
        dt = datetime.now() - t0
        acc = n_correct / n_total
        #document.querySelector("#recaptcha-anchor").click()
        print(
            f"Epoch : {e+1}/{epochs} Train_loss:{train_loss:.3f} Validation_loss:{validation_loss:.3f} Duration:{dt} Validation_Acc:{acc*100:.2f}"
        )

        if validation_loss < prev_loss:
          prev_loss = validation_loss
          #os.chdir('/content/drive/MyDrive')  #colab
          #os.chdir('/kaggle/working')  #kaggle
          os.chdir('C:/Local Machine/Cassava plant disease detection/...saves1')
          torch.save(model.state_dict() , 'plant_disease_model_14.pt')
          torch.save(train_dl , 'train_dl14.pth')
          torch.save(val_dl , 'val_dl14.pth')
          torch.save(test_dl , 'test_dl14.pth')
          print("model saved !")

    return train_losses, validation_losses

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 10)

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 10)

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 20)

"""lr     0.0005 ---------------> 0.001"""

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 20)

"""lr 0.001 ---------> 0.0005"""

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 2)

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 10)

model.train()
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 70)

optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 60)

"""Lr  0.0005 --------------> 0.00003"""

optimizer = torch.optim.Adam(model.parameters(),lr=0.00003)
train_losses, validation_losses = batch_gd(model, criterion, train_dl, val_dl, 5)

os.chdir('/content/drive/MyDrive')       #colab
#os.chdir('/kaggle/working')  #kaggle
torch.save(model.state_dict() , 'plant_disease_model_14.pt')

os.chdir('/content/drive/MyDrive')   #colab
#os.chdir('/kaggle/working')  #kaggle
torch.save(train_dl , 'train_dl14.pth')
torch.save(val_dl , 'val_dl14.pth')
torch.save(test_dl , 'test_dl14.pth')

#load model
os.chdir('C:/Local Machine/Cassava plant disease detection/...saves1')
#os.chdir('/content/drive/MyDrive')  #colab
#os.chdir('/kaggle/working')   #kaggle
targets_size = 5

# *******************************model = torch.load(f="plant_disease_model_9.pt")
model.load_state_dict(torch.load("plant_disease_model_14.pt"))
"""device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
device = "cuda"
model.to(device)"""
train_dl = torch.load("train_dl14.pth")
valn_dl = torch.load("val_dl14.pth")
test_dl = torch.load("test_dl14.pth")

plt.plot(train_losses , label = 'train_loss')
plt.plot(validation_losses , label = 'validation_loss')
plt.xlabel('No of Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

def accuracy(loader):
    n_correct = 0
    n_total = 0
    confusion_matrix = torch.zeros(targets_size,targets_size,dtype=torch.int64)

    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        _, predictions = torch.max(outputs, 1)
        
        for t, p in zip(targets.view(-1), predictions.view(-1)):
                confusion_matrix[t.long(), p.long()] += 1
        #print(targets.size(),outputs.size())
        #cm_data[targets.item() ,outputs.item()] += 1
        n_correct += (predictions == targets).sum().item()
        n_total += targets.shape[0]
        
    acc = n_correct / n_total
    #print(targets.item(2))
    #print(outputs.item(2))
    return acc,confusion_matrix

model.eval()
#torch.no_grad()
#os.chdir('/content')
os.chdir('C:/Local Machine/Cassava plant disease detection')

train_acc,train_cm = accuracy(train_dl)
print(f"Train Accuracy : {train_acc*100}% \n")
print(train_cm)

validation_acc,validation_cm = accuracy(val_dl)
print(f"Validation Accuracy : {validation_acc*100}% \n")
print(validation_cm)

test_acc,test_cm = accuracy(test_dl)
print(f"Test Accuracy : {test_acc*100}% \n")
print(test_cm)

def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

plt.figure(figsize=(10,10))
plot_confusion_matrix(train_cm, label_names)

plt.figure(figsize=(10,10))
plot_confusion_matrix(validation_cm, label_names)

plt.figure(figsize=(10,10))
plot_confusion_matrix(test_cm, label_names)

def show_img(model):
    model=np.reshape(model.numpy(),[224,224,3]) # For 1D Vector
    
    #If you normalize the image then use Next three-line
    #Otherwise skip that
    mean=np.array([0.485, 0.456, 0.406] )
    std=np.array([0.229, 0.224, 0.225])
    model=(model*std+mean)
    


    #print(model)

    plt.imshow("ABC", model)
    
    #waits for user to press any key
    #(this is necessary to avoid Python kernel form crashing)
    plt.waitKey(0)

    #closing all open windows
    plt.destroyAllWindows()


def copy_missclassified(dataset,loader):
    os.chdir('C:/Local Machine/Cassava plant disease detection')
    for i in label_names:
      for j in label_names:
        if i != j :
          if os.path.isdir(str(i)+'_'+str(j))==False:
            os.mkdir(str(i)+'_'+str(j))
    model.eval()      
    with torch.no_grad():
      for inputs, targets in loader:
          inputs, targets = inputs.to(device), targets.to(device)
          outputs = model(inputs)
          _, predictions = torch.max(outputs, 1)
          for sampleno in range(inputs.shape[0]):
              if(targets[sampleno]!=predictions[sampleno]):
                    show_img(inputs[sampleno].cpu())
          return predictions
          """
        
        #for t, p in zip(targets.view(-1), predictions.view(-1)):
        print(inputs)
            #confusion_matrix[t.long(), p.long()] += 1   """

copy_missclassified(pathology_train,val_dl)

def test(dataset, dataloader):
    net.eval()
    with torch.no_grad():
        for batch in dataloader:
            inputs = batch[0]
            label=batch[1]
            inputs = inputs.to(device, non_blocking=True)
            outputs = net(inputs)
            predictions = torch.argmax(outputs, dim=1)
            for sampleno in range(batch[0].shape[0]):
                if(label[sampleno]!=predictions[sampleno]):
                    print("Actual Lable")
                    print(label[sampleno])
                    print("Predicted Label")
                    print(predictions[sampleno])
                    showimg(inputs[sampleno].cpu())
            return predictions

def show_img(model):
    model=np.reshape(model.numpy(),[28,28]) # For 1D Vector
    
    #If you normalize the image then use Next three-line
    #Otherwise skip that
    mean=np.array([0.485, 0.456, 0.406] )
    std=np.array([0.229, 0.224, 0.225])
    model=(model*std+mean)
    


    #print(model)

    cv2.imshow("ABC", model)
    
    #waits for user to press any key
    #(this is necessary to avoid Python kernel form crashing)
    cv2.waitKey(0)

    #closing all open windows
    cv2.destroyAllWindows()

original = r'original path where the file is currently stored\file name.file extension'
target = r'target path where the file will be copied\file name.file extension'

shutil.copyfile(original, target)

copy_missclassified(test_dl)